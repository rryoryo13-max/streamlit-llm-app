
from dotenv import load_dotenv
import os

load_dotenv()

import streamlit as st

# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
if not os.getenv("OPENAI_API_KEY"):
    st.error("ğŸ”‘ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.info("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

st.title("è‚²å…ä»•äº‹ç”¨")
st.write("##### å‹•ä½œãƒ¢ãƒ¼ãƒ‰1: è‚²å…ç›¸è«‡")
st.write("è‚²å…ç›¸è«‡ãŒã§ãã¾ã™ã€‚")
st.write("##### å‹•ä½œãƒ¢ãƒ¼ãƒ‰2: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°")
st.write("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ç›¸è«‡ãŒã§ãã¾ã™ã€‚")

nutrition__template = """
ã‚ãªãŸã¯è¦ªã®è‚²å…ã®å°‚é–€å®¶ã§ã™ã€‚
è‚²å…ç–²ã‚Œã‚„ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ã«é–¢ã™ã‚‹å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
è¦ªè‡ªèº«ã®å¿ƒèº«ã®å¥åº·ã‚’ä¿ã¤ãŸã‚ã®æ–¹æ³•ã‚’æ•™ãˆã¾ã™ã€‚
è³ªå•ï¼š{input}
"""

Programming_template = """
ã‚ãªãŸã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«é–¢ã™ã‚‹è³ªå•ã‚„ç›¸è«‡ã«ä¹—ã‚Šã¾ã™ã€‚
è³ªå•ï¼š{input}
"""

# LLMã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹é–¢æ•°
def get_llm_response(input_text, selected_mode):
    try:
        from langchain_openai import OpenAI
        from langchain.prompts import PromptTemplate
        from langchain.chains import LLMChain
    except ImportError:
        # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å ´åˆ
        try:
            from langchain.llms import OpenAI
            from langchain.prompts import PromptTemplate
            from langchain.chains import LLMChain
        except ImportError:
            st.error("LangChainãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚requirements.txtã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return "ã‚¨ãƒ©ãƒ¼: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    
    # LLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    llm = OpenAI(temperature=0.7)
    
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ
    if selected_mode == "è‚²å…ç›¸è«‡":
        template = nutrition__template
    else:  # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°
        template = Programming_template
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
    prompt = PromptTemplate(
        input_variables=["input"],
        template=template
    )
    
    # ãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—
    response = chain.run(input=input_text)
    return response

selected_item = st.radio(
    "å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚",
    ["è‚²å…ç›¸è«‡", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"]
)

st.divider()

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
input_text = st.text_area(
    label="è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
    placeholder="ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
    height=100
)

if st.button("å®Ÿè¡Œ"):
    st.divider()
    
    if input_text.strip():
        # LLMã‹ã‚‰ã®å›ç­”ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            try:
                response = get_llm_response(input_text, selected_item)
                st.success("å›ç­”:")
                st.write(response)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    else:
        st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã‹ã‚‰ã€Œå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")